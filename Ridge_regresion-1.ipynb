{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozD4G4_de2Zz"
      },
      "source": [
        "# La regresión ridge \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUr7kH8svaku"
      },
      "source": [
        "## Qué es el overfitting\n",
        "\n",
        "Empecemos hablando de uno de los problemas más comunes en machine learning: el **riesgo de sobre-ajustar**, o en inglés, de hacer overfitting. Cuando construimos un modelo, tenemos que tener en cuenta dos factores: por un lado, queremos que las predicciones que nos de el modelo sean lo más precisas posible, es decir, que **el modelo tenga el menor error posible**. Sin embargo, por otro lado sabemos que **las observaciones de nuestro conjunto de datos seguramente tengan errores**: o bien errores de medida, o bien o estén influenciadas por algún aspecto que no tenemos en cuenta en el modelo. Encontramos estos errores por ejemplo:\n",
        "* En un conjuntos de datos con información biomédica, las máquinas usadas para tomar las mediciones introducen errores.\n",
        "* En un conjunto de datos con información sobre el precio de venta de las casas, este precio puede verse influido por variables que no aparezcan en el conjunto de datos.\n",
        "\n",
        "Así que con nuestro modelo queremos conseguir un compromiso, **queremos que sea capaz de captar la tendencia general de los datos, sin dejarse influir por los errores especificos** del conjunto con el que lo entrenamos. Con un ejemplo se entiende todo mejor,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1rzmk-_eqyl"
      },
      "source": [
        "![Relación lineal vs relación no lineal](http://blog.noaxacademy.com/wp-content/uploads/2021/12/ejemplo_overfitting.svg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2zV9ALf37-d"
      },
      "source": [
        "**Figura 1 |** A la izquierda, modelo infra ajustado, en el centro, modelo sobre ajustado, a la derecha modelo bien ajustado.\n",
        "\n",
        "Imagina que tenemos el conjunto de datos que sigue la tendencia curva de la imagen superior y queremos construir un modelo para hacer predicciones. \n",
        "\n",
        "* Si nuestro modelo es muy general, estaremos en la situación de la imagen izquierda, haciendo underfitting, y no estaremos captando realmente el comportamiento de los datos.\n",
        "\n",
        "* Pero si nuestro modelo es demasiado poco general, estaremos en la situación de la imagen central, haciendo overfitting, y estaremos incluyendo en el modelo los errores de las observaciones. Es decir, un cambio en nuestro conjunto de datos producirá un cambio muy grande en el modelo.\n",
        "\n",
        "* Está claro que el mejor modelo es el de la imagen de la derecha, porque capta el comportamiento general (esa tendencia curva) sin dejarse influir por todos los errores específicos de las observaciones.\n",
        "\n",
        "Ahora bien, la varianza en un modelo nos dice cómo de variable es el modelo. Cuanta mayor es la varianza, más cambios bruscos, subidas y bajadas veremos en las predicciones del modelo. Si la varianza es muy pequeña, el modelo es muy estable y está infra ajustado. Pero si es muy grande, el modelo es demasiado variable y está sobre ajustado. Buscamos un punto medio entre las dos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po-TcGle9Apm"
      },
      "source": [
        "## Qué es el sesgo\n",
        "\n",
        " El sesgo mide la distancia que hay entre el valor esperado de nuestro modelo y el verdadero valor,\n",
        "\n",
        "$$\\text{sesgo}(\\hat\\beta)=\\mathbb{E}(\\hat\\beta)-\\beta$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqSNW61E3zok"
      },
      "source": [
        "## El mejor estimador insesgado..\n",
        "\n",
        "Pues bien, veamos cómo se relaciona todo esto con la regresión lineal. La ecuación del modelo de regresión es,\n",
        "\n",
        "$$y = X\\beta + \\varepsilon$$\n",
        "\n",
        "Donde $y$ es el vector de la variable respuesta (la que queremos predecir), $X$ es la matrz con las variables predictoras (que usaremos para hacer las predicciones) y $\\beta$ es el vector con los parámetros del modelo de regresión (los valores de la ecuación de la recta). Recordemos que para encontrar la solución óptima del modelo de regresión resolvíamos la ecuación,\n",
        "\n",
        "$$\n",
        "J(\\beta)=\\text{min}\\left\\{\\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{y}_i)^2\\right\\}\n",
        "$$\n",
        "\n",
        "Si no recuerdas de donde viene esta fórmula no te preocupes, puedes leerlo [en el post anterior](https://blog.noaxacademy.com/el-abc-del-machine-learning-regresion-lineal-con-python/). No quiero entrar en muchos detalles técnicos, pero es importante saber que matemáticamente, la solución del modelo de regresión lineal es el **mejor estimador lineal insesgado**, es decir, es el que tiene la menor varianza de entre todos los posibles tipos de modelos lineales insesgados. Esto en principio es bueno... ¿o no?\n",
        "\n",
        "Aquí entra en juego el equilibrio entre el sesgo y la varianza, lo que en inglés se llama el **variance bias tradeoff**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSNW6RLOTKfy"
      },
      "source": [
        "## El equilibrio entre sesgo y varianza\n",
        "\n",
        "Cuando se construye un modelo de machine learning, existe un equilibrio entre su sesgo y su varianza. \n",
        "* Si el sesgo disminuye, la varianza aumenta;\n",
        "* Si la varianza disminuye, el sesgo aumenta.\n",
        "\n",
        "Y además, también se sabe que el aumentar la complejidad del modelo, es decir, **aumentar el número de variables** aumenta la varianza (y disminuye el sesgo). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jug93IbwV5vH"
      },
      "source": [
        "\n",
        "\n",
        "## La penalización ridge\n",
        "\n",
        "Y aquí llegamos a **la clave para entender la regresión ridge**: Hemos dicho que el modelo de regresión normal es el mejor estimador insesgado, pero ¿puede ser que consigamos mejores predicciones con un modelo que tenga un poco de sesgo, pero que a cambio reduzca mucho la varianza?\n",
        "\n",
        "La respuesta, como te puedes imaginar, es si, y eso es precisamente lo que hace la penalización ridge. Matemáticamente, los modelos ridge resuelven la ecuación,\n",
        "\n",
        "$$\n",
        "J(\\beta)=\\text{min}\\left\\{\\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{y}_i)^2+\\alpha\\sum_{j=1}^p\\beta_j^2\\right\\}\n",
        "$$\n",
        "\n",
        "Esta ecuación tiene dos partes, la primera parte es la misma que en un modelo de regresión no penalizado, y la segunda parte es la que se conoce como **penalización ridge**. La solución del modelo ridge serán aquellos valores de $\\beta$ que minimicen la función $J(\\beta)$.\n",
        "\n",
        "**¿Pero cómo funciona esta penalización?** Si los valores de $\\beta$ fuesen muy grandes, el valor de la penalización ridge sería muy grande, pero lo que queremos es buscar el valor mínimo de $J(\\beta)$. Es decir, que con esta penalización estamos \"forzando\" a que los coeficientes $\\beta$ no sean excesivamente grandes. De forma intuitiva, valores pequeños de los coeficientes harán que la varianza sea menor. \n",
        "\n",
        "## Optimización del modelo\n",
        "\n",
        "Fíjate por último en ese $\\alpha$ que aparece en la fórmula de la penalización. Eso es lo que se conoce como un hiper parámetro del modelo de regresión, y es un coeficiente que nosotros, como analistas de datos, debemos ajustar. Valores muy grandes de $\\alpha$ hacen que la importancia de la penalización sea mayor, y que los coeficientes $\\beta$ se hagan más pequeños para conseguir minimizar $J(\\beta)$, y valores muy pequeños de $\\alpha$ hacen que la penalización no tenga tanta importancia. De hecho, si $\\alpha=0$ (este es el valor más pequeño que puede tomar) estaremos resolviendo un modelo no penalizado.\n",
        "\n",
        "En la práctica, para encontrar el valor óptimo de $\\alpha$ suele considerarse una lista de valores posibles, y se resuelve el modelo con todos ellos. Por último se selecciona el $\\alpha$ que minimice el error.\n",
        "\n",
        "\n",
        "Veamos cómo resolver un modelo de regresión ridge en Python. Para ello usaremos un conjunto de datos de $442$ pacientes con diabetes. En este conjunto de datos se midieron $10$ variables, la edad, sexo, índice de masa corporal, presión sanguínea, y otras seis mediciones de serum en sangre, así como una variable adicional que mide el progreso de la enfermedad en los pacientes. Nuestro objetivo es usar las $10$ variables predictoras para construir un modelo que prediga el progreso de la enfermedad.\n",
        "\n",
        "En primer lugar cargamos las librerías de python que utilizaremos en esta sección:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liCw2Pjf5nzn"
      },
      "source": [
        "import numpy as np \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44tyC6oze0RR"
      },
      "source": [
        "Ahora cargamos los datos usando la función `load_diabetes()`, y los escalamos para que tengan media $0$ y desviación típica $1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWSmTE3ML0mb"
      },
      "source": [
        "diabetes = load_diabetes()\n",
        "x = diabetes.data\n",
        "y = diabetes.target\n",
        "\n",
        "# Split into train / test\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=100, random_state=42)\n",
        "\n",
        "# Scale\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAf0J4Iqfoyg"
      },
      "source": [
        "Construimos en primer lugar un modelo de regresión no penalizado usando la parte de entrenamiento (train) de nuestros datos, y después evaluamos el modelo calculando su error cuadrático medio en la parte de test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qdCpghAIWkb"
      },
      "source": [
        "linear_model = LinearRegression()\n",
        "linear_model.fit(x_train, y_train)\n",
        "\n",
        "lm_prediction = linear_model.predict(x_test)\n",
        "lm_error = mean_squared_error(y_test, lm_prediction)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWn7_6XDf1iP"
      },
      "source": [
        "Ahora vamos a construir los modelos de regresión ridge. Para eso definimos un vector de posibles valores para $\\alpha$. Lo habitual con estos parámetros es definirlos usando escala logarítmica. En este ejemplo consideramos 500 posibles valores entre $10^{-10}$ y $10^2$. Para cada posible valor de $\\alpha$, entrenamos el modelo en la parte de train, y lo testeamos en la parte de test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wyqk-ob0JReE"
      },
      "source": [
        "alpha = 10**np.linspace(-10, 2, 500)\n",
        "error_array = np.zeros(len(alpha))\n",
        "\n",
        "for idx, a in enumerate(alpha):\n",
        "  ridge_regression = Ridge(alpha=a)\n",
        "  ridge_regression.fit(x_train, y_train)\n",
        "  prediction = ridge_regression.predict(x_test)\n",
        "  error_array[idx] = mean_squared_error(y_test, prediction)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SycSraI1huOb",
        "outputId": "1ae5e36f-c604-4bf2-a77c-7191fe259dfe"
      },
      "source": [
        "# Optimal alpha\n",
        "smallest_error = np.min(error_array)\n",
        "optimal_alpha = alpha[np.argmin(error_array)]\n",
        "\n",
        "print(f'El valor óptimo de alpha es: {np.round(optimal_alpha, 2)}, con el que se obtiene un error de {np.round(smallest_error, 2)}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El valor óptimo de alpha es: 26.48, con el que se obtiene un error de 2684.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZsVMJX0gg8G"
      },
      "source": [
        "Por último vamos a representar los errores que hemos obtenido con los distintos modelos. En el eje vertical representamos el error, y en el eje horizontal representamos el hiper parámetro $\\alpha$. La línea verde es el error que tenemos usando el modelo de regresión no penalizado, y la línea azul es el error de los modelos ridge en función del valor de $\\alpha$. Como podemos ver, usando un modelo de regresión ridge conseguimos un error de predicción menor que con un modelo no penalizado para valores de $\\alpha$ menores de 80 (aproximadamente), y el valor óptimo de $\\alpha$ con el que se obtiene el menor error es 26.48.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "8tXXvtgzQcSi",
        "outputId": "12e02781-44d7-4acb-8943-f0e96a4f869e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(alpha, error_array)\n",
        "plt.axhline(y=lm_error, color='g', linestyle='--')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.lines.Line2D at 0x7fc78c9b8b90>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}